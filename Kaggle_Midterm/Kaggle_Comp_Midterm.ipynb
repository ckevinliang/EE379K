{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import sympy\n",
    "import scipy\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import collections\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix\n",
    "from sklearn.cross_decomposition import PLSRegression, PLSSVD\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "import xgboost as xgb\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"D:\\Kevin Liang/Documents/1_UT_SENIOR/UT_AUSTIN_FALL_2017/EE_379K/Kaggle_Comp_Midterm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_final = pd.read_csv(path + \"test_final.csv\", header = 0)\n",
    "train_data = pd.read_csv(path + \"train_final.csv\", header = 0)\n",
    "\n",
    "# retrieve ID column for creating submission CSV\n",
    "ID = X_final[\"id\"]\n",
    "\n",
    "# create X test\n",
    "X_final = X_final.drop([\"id\"], axis = 1)\n",
    "    \n",
    "# replace NaN with mean of column\n",
    "train_data = train_data.fillna(train_data.mean())\n",
    "X_final = X_final.fillna(train_data.mean())\n",
    "\n",
    "# create X train and Y train\n",
    "X = train_data.drop([\"Y\",\"id\"], axis = 1)\n",
    "y = train_data.loc[:,\"Y\"]\n",
    "\n",
    "# calculated correlation matrix to find correlations between features\n",
    "data_corr = train_data.corr()\n",
    "\n",
    "#for col_name in X.columns:\n",
    "#    X = remove_outliers(data = X, column = str(col_name), mean = X[col_name].mean(), std = X[col_name].std())\n",
    "\n",
    "#print data_corr\n",
    "#for x in train_data.columns:\n",
    "#    sns.distplot(train_data[x])\n",
    "#    sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Regressor Score: 0.859195005372\n"
     ]
    }
   ],
   "source": [
    "# XGBoost from previous lab\n",
    "\n",
    "parameters = {\n",
    "    'max_depth':[1, 2, 6, 8, 10],\n",
    "    'n_estimators':[25, 50, 100, 150, 200],\n",
    "    'reg_alpha' : [0.001, 0.05, 0.1, 0.25, 0.5]\n",
    "}\n",
    "\n",
    "xgmodel = xgb.XGBRegressor()\n",
    "#xg_clf = GridSearchCV(xgmodel, parameters, cv = 5, scoring = \"roc_auc\").fit(X, y)\n",
    "\n",
    "cv_score = cross_val_score(xgmodel, X, y, cv=5, scoring = \"roc_auc\").mean()\n",
    "print \"XGBoost Regressor Score: \" + str(cv_score)\n",
    "\n",
    "xgmodel = xgmodel.fit(X,y)\n",
    "y_pred = xgmodel.predict(X_final)\n",
    "\n",
    "# public lb 0.86326\n",
    "# local roc auc 0.858436561237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeCV Score: 0.688477725342\n",
      "Optimal RidgeCV alpha: 150.0\n",
      "\n",
      "LassoCV Score: 0.688909950315\n",
      "Optimal LassoCV alpha: 150.0\n",
      "\n",
      "LinearRegression Score: 0.687750320969\n",
      "\n",
      "LogisticRegression Score: 0.634608940378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75, 100,150,200,210,220,230,240,250,300,400]\n",
    "r_cv = RidgeCV(alphas = alphas)\n",
    "cv_score = cross_val_score(r_cv, X, y, cv=5, scoring = \"roc_auc\").mean()\n",
    "print \"RidgeCV Score: \" + str(cv_score)\n",
    "r_cv = r_cv.fit(X, y)\n",
    "print \"Optimal RidgeCV alpha: \" + str(r_cv.alpha_) + '\\n'\n",
    "y_pred = r_cv.predict(X_final)\n",
    "\n",
    "# Lasso\n",
    "alphas = [10, 1, 0.1, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005]\n",
    "l_cv = LassoCV(alphas = alphas)\n",
    "cv_score = cross_val_score(l_cv, X, y, cv=5, scoring = \"roc_auc\").mean()\n",
    "print \"LassoCV Score: \" + str(cv_score)\n",
    "l_cv = l_cv.fit(X, y)\n",
    "print \"Optimal LassoCV alpha: \" + str(r_cv.alpha_) + '\\n'\n",
    "y_pred = l_cv.predict(X_final)\n",
    "\n",
    "# Logistic \n",
    "lin_cv = LinearRegression()\n",
    "cv_score = cross_val_score(lin_cv, X, y, cv=10, scoring = \"roc_auc\").mean()\n",
    "print \"LinearRegression Score: \" + str(cv_score) + '\\n'\n",
    "lin_cv = lin_cv.fit(X,y)\n",
    "y_pred = lin_cv.predict(X_final)\n",
    "\n",
    "log_cv = LogisticRegression()\n",
    "cv_score = cross_val_score(log_cv, X, y, cv=10, scoring = \"roc_auc\").mean()\n",
    "print \"LogisticRegression Score: \" + str(cv_score) + '\\n'\n",
    "lin_cv = log_cv.fit(X,y)\n",
    "y_pred = log_cv.predict(X_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor CV Score: 0.771096376759\n",
      "\n",
      "XBGRegressor CV Score: 0.858935216142\n"
     ]
    }
   ],
   "source": [
    "rf_reg = RandomForestRegressor(random_state = 12323)\n",
    "cv_score = cross_val_score(rf_reg, X, y, cv=10, scoring = \"roc_auc\").mean()\n",
    "print \"RandomForestRegressor CV Score: \" + str(cv_score) + '\\n'\n",
    "\n",
    "xg_reg = xgb.XGBRegressor()\n",
    "cv_score = cross_val_score(xg_reg, X, y, cv=10, scoring = \"roc_auc\").mean()\n",
    "print \"XBGRegressor CV Score: \" + str(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Components / Negative Cross Val Score / Variance\n",
      "Number of Components:1\n",
      "ROC AUC Score: 0.617352506592\n",
      "\n",
      "Number of Components:2\n",
      "ROC AUC Score: 0.799235945921\n",
      "\n",
      "Number of Components:3\n",
      "ROC AUC Score: 0.799484327524\n",
      "\n",
      "Number of Components:4\n",
      "ROC AUC Score: 0.803292533717\n",
      "\n",
      "Number of Components:5\n",
      "ROC AUC Score: 0.802676750754\n",
      "\n",
      "Number of Components:6\n",
      "ROC AUC Score: 0.802821042899\n",
      "\n",
      "Number of Components:7\n",
      "ROC AUC Score: 0.802502924668\n",
      "\n",
      "Number of Components:8\n",
      "ROC AUC Score: 0.802878454172\n",
      "\n",
      "Number of Components:9\n",
      "ROC AUC Score: 0.806772849115\n",
      "\n",
      "Number of Components:10\n",
      "ROC AUC Score: 0.806862892876\n",
      "\n",
      "Number of Components:11\n",
      "ROC AUC Score: 0.806690189882\n",
      "\n",
      "Number of Components:12\n",
      "ROC AUC Score: 0.806368549663\n",
      "\n",
      "Number of Components:13\n",
      "ROC AUC Score: 0.830532502471\n",
      "\n",
      "Number of Components:14\n",
      "ROC AUC Score: 0.830343718606\n",
      "\n",
      "Number of Components:15\n",
      "ROC AUC Score: 0.829968971047\n",
      "\n",
      "Number of Components:16\n",
      "ROC AUC Score: 0.829912835948\n",
      "\n",
      "Number of Components:17\n",
      "ROC AUC Score: 0.834043018217\n",
      "\n",
      "Number of Components:18\n",
      "ROC AUC Score: 0.835627348124\n",
      "\n",
      "Number of Components:19\n",
      "ROC AUC Score: 0.835663015505\n",
      "\n",
      "Number of Components:20\n",
      "ROC AUC Score: 0.835551581801\n",
      "\n",
      "Number of Components:21\n",
      "ROC AUC Score: 0.84064848568\n",
      "\n",
      "Number of Components:22\n",
      "ROC AUC Score: 0.844099093234\n",
      "\n",
      "Number of Components:23\n",
      "ROC AUC Score: 0.843891047444\n",
      "\n",
      "Number of Components:24\n",
      "ROC AUC Score: 0.858318675299\n",
      "\n",
      "Number of Components:25\n",
      "ROC AUC Score: 0.858318675299\n",
      "\n",
      "Number of Components:26\n",
      "ROC AUC Score: 0.859195005372\n",
      "\n",
      "Number of Components:27\n",
      "ROC AUC Score: 0.859195005372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PCR (PCA + linearRegression)\n",
    "\n",
    "components = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27]\n",
    "\n",
    "component = 0\n",
    "total_variance = 0\n",
    "cvScore = 0\n",
    "print \"# of Components / Negative Cross Val Score / Variance\"\n",
    "for x in components:\n",
    "    pca = PCA(n_components = x, svd_solver = \"full\")\n",
    "    cv_score = cross_val_score(pca, X, y, cv=10).mean()\n",
    "    pca  = pca.fit(X, y)\n",
    "    #print x, cv_score, pca.explained_variance_ratio_[-1]\n",
    "    if total_variance < .9:\n",
    "        total_variance += pca.explained_variance_ratio_[-1]\n",
    "        component = x\n",
    "        cvScore = cv_score\n",
    "\n",
    "for x in components:\n",
    "    r_cv = xgb.XGBRegressor()\n",
    "    r_cv = r_cv.fit(X.iloc[:,:x+1], y)\n",
    "    X_test_pcr = X_final.iloc[:,:x+1]\n",
    "    y_pred = r_cv.predict(X_test_pcr)\n",
    "    cv_score = cross_val_score(r_cv, X.iloc[:,:x+1], y, cv=5, scoring = \"roc_auc\").mean()\n",
    "    print \"Number of Components:\" + str(x)\n",
    "    print \"ROC AUC Score: \" + str(cv_score) +'\\n'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking attempt lol (failed)\n",
    "\n",
    "alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75] \n",
    "r_cv = RidgeCV(alphas = alphas)\n",
    "r_cv = r_cv.fit(X, y)\n",
    "y_pred_R = r_cv.predict(X_final)\n",
    "\n",
    "alphas = [10, 1, 0.1, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005]\n",
    "l_cv = LassoCV(alphas = alphas)\n",
    "l_cv = l_cv.fit(X, y)\n",
    "y_pred_L = l_cv.predict(X_final)\n",
    "\n",
    "X_final['Ridge Predictions'] = pd.Series(y_pred_R, index=X_final.index)\n",
    "X_final['Lasso Predictions'] = pd.Series(y_pred_L, index= X_final.index)\n",
    "y_pred_R = y_pred_R[:len(y_pred_R) -2:]\n",
    "y_pred_L = y_pred_L[:len(y_pred_L) -2:]\n",
    "X['Ridge Predictions'] = pd.Series(y_pred_R, index = X.index)\n",
    "X['Lasso Predictions'] = pd.Series(y_pred_L, index = X.index)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.86061, std: 0.00853, params: {'scale_pos_weight': 0.1}, mean: 0.86077, std: 0.00843, params: {'scale_pos_weight': 0.5}, mean: 0.86100, std: 0.00844, params: {'scale_pos_weight': 1}, mean: 0.86117, std: 0.00873, params: {'scale_pos_weight': 2}, mean: 0.86119, std: 0.00865, params: {'scale_pos_weight': 3}] {'scale_pos_weight': 3} 0.861192713358\n"
     ]
    }
   ],
   "source": [
    "param_test1 = {\n",
    "    'scale_pos_weight':[0.1,0.5,1,2,3]\n",
    "    \n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBRegressor( learning_rate =0.01, n_estimators=1100, max_depth=3,\n",
    " min_child_weight=3, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27, base_score = 0.3, max_delta_step = 1), \n",
    "                       param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "gsearch1.fit(X,y)\n",
    "y_pred = gsearch1.predict(X_final)\n",
    "print gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
    "\n",
    "\n",
    "#{'max_depth': 3, 'min_child_weight': 4},\n",
    "#0.86023918063345839\n",
    "\n",
    "#{'max_depth': 3, 'min_child_weight': 3},\n",
    "# 0.86080023248484605)\n",
    "\n",
    "#{'subsample': 0.8, 'colsample_bytree': 0.8}\n",
    "# 0.861038005534\n",
    "\n",
    "#{'reg_alpha': 1e-05} \n",
    "#0.86103803692\n",
    "\n",
    "#{'n_estimators': 110} \n",
    "#0.861092907843\n",
    "\n",
    "#{'n_estimators': 120, 'reg_alpha': 0.001} 0.837285316704\n",
    "\n",
    "#{'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 3} 0.861206345813\n",
    "\n",
    "#{'n_estimators': 1100, 'learning_rate': 0.01} 0.861236793201\n",
    "\n",
    "#{'scale_pos_weight': 2} 0.861321702375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used this one cell to tune the hyperparameters for the XGBRegressor. I would run with set hyperparameters and record the local CV score. I would then tune the parameters and run others. I only recorded scores that imporved previous scores which are shown in the comments of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.86105, std: 0.00769, params: {'reg_alpha': 1}] {'reg_alpha': 1} 0.861052729409\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier\n",
    "param_test2 = {\n",
    " 'reg_alpha':[1]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.01, n_estimators=1000, max_depth=3,\n",
    " min_child_weight=3, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27,reg_alpha = 1), \n",
    "                       param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "gsearch2.fit(X,y)\n",
    "y_pred = gsearch2.predict(X_final)\n",
    "print gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_\n",
    "\n",
    "# {'max_depth': 2, 'min_child_weight': 3} 0.8572636502\n",
    "# {'n_estimators': 120} 0.860541566744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Val Score(stacked): 0.801978687251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stacking with StackingRegressor\n",
    "r_alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75] \n",
    "l_alphas = [10, 1, 0.1, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005]\n",
    "lr = LinearRegression()\n",
    "logr = LogisticRegression()\n",
    "r_cv = RidgeCV(alphas = r_alphas)\n",
    "l_cv = LassoCV(alphas = l_alphas)\n",
    "xgb1 = XGBRegressor(seed = 0, nthread = -1, colsample_bytree = 0.69, subsample = 0.69, learning_rate = 0.1, \n",
    "        max_depth = 6, min_child_weight = 1, n_estimators = 100)\n",
    "xgb2 = ExtraTreesRegressor(random_state = 0, n_jobs = -1, n_estimators = 100, \n",
    "        max_features = 0.49, max_depth = 8, min_samples_leaf = 2)\n",
    "xgb3 = RandomForestRegressor(random_state = 0, n_jobs = -1, n_estimators = 100, \n",
    "        max_features = 0.22, max_depth = 8, min_samples_leaf = 2)\n",
    "lin_r = XGBRegressor(seed = 0, nthread = -1, colsample_bytree = 0.69, subsample = 0.69, learning_rate = 0.1, \n",
    "        max_depth = 6, min_child_weight = 1, n_estimators = 100)\n",
    "\n",
    "stregr = StackingRegressor(regressors=[lr, logr, r_cv, l_cv, xgb1, xgb2, xgb3], \n",
    "                           meta_regressor=lin_r)\n",
    "print \"Cross Val Score(stacked): \" + str(cross_val_score(stregr, X, y, cv=5, scoring = \"roc_auc\").mean()) + '\\n'\n",
    "y_pred = stregr.fit(X,y).predict(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predictions = pd.DataFrame(y_pred, columns = [\"id\", \"Y\"])\n",
    "#predictions.to_csv(\"predictions.csv\")\n",
    "predictions = pd.DataFrame({\"id\":ID, \"Y\":y_pred})\n",
    "predictions = predictions[predictions.columns[::-1]]\n",
    "predictions.to_csv(\"predictions.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
